{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DF1RARDqNUe"
      },
      "source": [
        "#  ESN on Lorenz 63"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNWWwbzAqcpB"
      },
      "source": [
        "## Instructions on Setting up Colab for JULIA\n",
        "\n",
        "1. Work on a copy of this notebook: _File_ > _Save a copy in Drive_ (you will need a Google account). Alternatively, you can download the notebook using _File_ > _Download .ipynb_, then upload it to [Colab](https://colab.research.google.com/).\n",
        "2. If you need a GPU: _Runtime_ > _Change runtime type_ > _Harware accelerator_ = _GPU_.\n",
        "3. Execute the following cell (click on it and press Ctrl+Enter) to install Julia, IJulia and other packages (if needed, update `JULIA_VERSION` and the other parameters). This takes a couple of minutes.\n",
        "4. Reload this page (press Ctrl+R, or ⌘+R, or the F5 key) and continue to the next section.\n",
        "\n",
        "_Notes_:\n",
        "* If your Colab Runtime gets reset (e.g., due to inactivity), repeat steps 2, 3 and 4.\n",
        "* After installation, if you want to change the Julia version or activate/deactivate the GPU, you will need to reset the Runtime: _Runtime_ > _Factory reset runtime_ and repeat steps 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qIMNMNYOqWUI",
        "outputId": "7dd5003b-1359-4d84-a293-0d3b04d26f98"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.11.2\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia PlotlyLight ReservoirComputing LinearAlgebra JLD2\"\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia\n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key)\"\n",
        "fi\n",
        "\n",
        "# Downloading data\n",
        "wget --no-cache --backups=1 \"https://raw.githubusercontent.com/llfung/ESN-workshop/main/data.jld2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7nwDMNfqh5Q"
      },
      "source": [
        "## Initialising the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XWhPgdL8qNUg",
        "outputId": "7c237b49-f6e1-436f-af31-4c6934e294d7"
      },
      "outputs": [],
      "source": [
        "# Install required packages if not installed yet\n",
        "import Pkg\n",
        "Pkg.add([\"PlotlyLight\",\"ReservoirComputing\",\"LinearAlgebra\",\"JLD2\"]);\n",
        "# Call the required packages\n",
        "# using OrdinaryDiffEq # For generating the data from ODEs\n",
        "using JLD2 # For loading pre-computed time-series data\n",
        "using PlotlyLight # For plotting\n",
        "\n",
        "import ReservoirComputing: rand_sparse # For generating the random sparse matrix\n",
        "\n",
        "import LinearAlgebra: I ; # For the identity matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7EJi40GqNUh"
      },
      "source": [
        "## Loading data from Lorenz 63 System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "vQt9BsmOqNUh",
        "outputId": "c2c869ac-212c-4c30-d792-859714ccc5ab"
      },
      "outputs": [],
      "source": [
        "## Loading Data\n",
        "@load \"data.jld2\"\n",
        "# Plot the solution\n",
        "plot.scatter(x = tdata, y=data[1,:],name = \"x1\").\n",
        "  scatter(x = tdata, y=data[2,:],name = \"x2\").\n",
        "  scatter(x = tdata, y=data[3,:],name = \"x3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlIKuYn5HF1"
      },
      "source": [
        "## Preparing the dataset\n",
        "There are three stages in training a echo state network:\n",
        "\n",
        "1. Washout\n",
        "   - The reservoir system $\\mathbf{r}$, driven by the input signal, takes some time (how long?) to sync with the driving signal. After the system achieve syncrony, presumably after this washout stage, the reservoir state $\\mathbf{r}(\\mathbf{x}(t))$ can be treated as a function of the input signal $\\mathbf{x}$.\n",
        "   - We train the reservoir system in the 'open loop' configuration.\n",
        "2. Training\n",
        "   - In this stage, we seek to find a mapping from $\\mathbf{r}(\\mathbf{x}(t))$ to the original signal $\\mathbf{x}(t)$. Assuming that the reservoir has a high enough dimension, this mapping, a.k.a. readout, can be **approximated** as linear. In other words, the readout is \"local\" to the $\\mathbf{r}(t)$ states we are training on.\n",
        "   - We keep the reservoir system in the 'open loop' configuration to generate a set of data that pairs between $\\mathbf{r}$ and $\\mathbf{x}(t)$.\n",
        "3. Testing\n",
        "   - In this stage, we test the predictive power of the trained network by comparing the prediction from the ESN with the original signal.\n",
        "   - We change the configuration into 'close loop'.\n",
        "\n",
        "To prepare for these three stages, we need to first split the time-series data into three chunks. Complete the following to split the data into three by indexing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "odpgtGbVqNUi",
        "outputId": "4fdd5788-60a1-410b-8fff-7de9d15e08f6"
      },
      "outputs": [],
      "source": [
        "# Size of dataset\n",
        "display(size(data)) # Size should be (system dimension x number of time points)\n",
        "(sys_dim, total_size) = size(data)\n",
        "\n",
        "## Preparing the data\n",
        "washout_size = 1000\n",
        "train_size = 4000\n",
        "test_size = total_size- washout_size - train_size\n",
        "\n",
        "washout_idx = 1:washout_size\n",
        "train_idx = (washout_size+1):(washout_size+train_size)\n",
        "test_idx = (washout_size+train_size+1):(washout_size+train_size+test_size) ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RhDBmbm6qNUj"
      },
      "outputs": [],
      "source": [
        "## Setting the hyperparameters\n",
        "reservoir_size = 1000\n",
        "\n",
        "# Tikh Regularization parameter\n",
        "γ = 1e-6 ;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0twPYYo0qNUj"
      },
      "source": [
        "## Initializing the ESN\n",
        "In this step, we set up the ESN by assigning random weights to the reservoir matrix, input signal matrix and signal bias. However, there are some requirements to this \"randomness\".\n",
        "\n",
        "### Reservoir matrix $W_r$\n",
        "The ESN needs to fulfill the fading memory requirement, that is, it needs to forget the initial state. To do that, after a random initialisation, we scale the matrix such that the spectral radius is below one. (How does the spectral radius relates to the length of the washout time?)\n",
        "\n",
        "### Input Signal matrix $W_{in}$\n",
        "There isn't a theoretical requirement to this matrix, but empirically we see better performance if each reservoir state only receive forcing from one of the dimension of the signal.\n",
        "\n",
        "### Signal Bias $\\mathbf{b}$\n",
        "There isn't a theoretical requirement to this matrix. In fact, close-looop prediction of ESN will still work if $\\mathbf{b}=0$. However, putting $\\mathbf{b}=0$ may prevent it from mapping $\\mathbf{x}$ to more general functions of $\\mathbf{x}$. Empirically, we see better performance if $b$ is initialise at the same scale as the signal (why?).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myAUslZrqNUj",
        "outputId": "b95efeae-d907-4a21-e800-be028b0ad102"
      },
      "outputs": [],
      "source": [
        "# Define the function to generate input signal matrix W_in\n",
        "function Win_gen(n, m) # n = reservoir_size, m = input signal dimensions\n",
        "    Win = zeros(n, m)\n",
        "    for i = 1:n\n",
        "        Win[i, rand(1:m)] = rand()*2.0-1.0\n",
        "    end\n",
        "    return Win\n",
        "end\n",
        "\n",
        "# Define the function to generate the reservoir matrix\n",
        "# (use function rand_sparse from ReservoirComputing.jl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLoeYDh_qNUk"
      },
      "source": [
        "Code to call the respective functions to generate the reservoir matrix $W_r$, input signal matrix $W_{in}$ and input signal bias $b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zazVeTcGqNUk"
      },
      "outputs": [],
      "source": [
        "# Setting the reservoir matrix (using function from ReservoirComputing.jl)\n",
        "W = rand_sparse(reservoir_size, reservoir_size,radius = 0.1) # Spectral radius = 0.1\n",
        "\n",
        "# Setting the input signal matrix\n",
        "W_in = Win_gen(reservoir_size, sys_dim)\n",
        "\n",
        "# Setting the input signal bias\n",
        "b = rand(reservoir_size,1)*(maximum(data)-minimum(data)) .+ minimum(data)\n",
        "\n",
        "# Initialize the memory for reservoir states (for faster computation)\n",
        "r = zeros(reservoir_size,washout_size+train_size+test_size);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeEHJUHPqNUl"
      },
      "source": [
        "## Running/training the ESN\n",
        "The evolution of a vanilla ESN is defined by the following equation:\n",
        "\n",
        "$$\n",
        "\\mathbf{r}_{t+1} = \\sigma{W_r \\mathbf{r}_t + W_{in} \\mathbf{x}_t + b}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{y}_{t} = W_{out,y} \\mathbf{r}_t\n",
        "$$\n",
        "\n",
        "There are two configurations to run an ESN:\n",
        "- Open Loop\n",
        "  - Driving signal $\\mathbf{x}$ is given.\n",
        "  - Driving signal $\\mathbf{x}$ is connected to the ESN (through $W_{in}$)and drives the evolution of the reservoir state $\\mathbf{r}$.\n",
        "- Close Loop\n",
        "  - Given a trained readout matrix $W_{out,x}$ for $\\mathbf{y}_t = \\mathbf{x}_t$, we evolve the whole ESN using the prediction of $\\mathbf{x}_t$ from the previous time step.\n",
        "  - Output signal $\\mathbf{y}_t$ is to be predicted by ESN.\n",
        "\n",
        "Complete the following functions to run the ESN in open and close loop configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WmriXd-ZqNUl"
      },
      "outputs": [],
      "source": [
        "function stepping(ri, U, W_in, W, b, g = tanh)\n",
        "    return g.(W_in*U + W*ri +b)\n",
        "end\n",
        "\n",
        "function open_loop!(r,W,W_in,b,U,ini_idx,fin_idx)\n",
        "    for i=ini_idx:fin_idx\n",
        "        r[:,i] = stepping(r[:,i-1], U[:,i-1], W_in, W, b)\n",
        "    end\n",
        "end\n",
        "\n",
        "function close_loop!(r,W,W_in,b,Ui,Wout,ini_idx,fin_idx)\n",
        "    for i=ini_idx:fin_idx\n",
        "        r[:,i] = stepping(r[:,i-1], Ui, W_in, W, b)\n",
        "        Ui = Wout*r[:,i]\n",
        "    end\n",
        "end ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMWwTS6pqNUm"
      },
      "source": [
        "Now, we can use the functions to run the ESN for the washout and training stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "y9C3sTdgqNUm"
      },
      "outputs": [],
      "source": [
        "## Training Echo State Network\n",
        "# Washout Stage : Running the ESN in open-loop from t=2 to t=washout_size (Initial Condition: r_{t=1} is set to zero)\n",
        "open_loop!(r,W,W_in,b,data,2,washout_size)\n",
        "\n",
        "# Training Stage : Running the ESN in open-loop from t=washout_size+1 to t=washout_size+train_size) (Initial Condition: r_{t=1} is set to zero)\n",
        "# Before feeding in the data, we add some artificial noise. This helps making the training more robust.\n",
        "# data[:,train_idx] .+= randn(size(data[:,train_idx]))*0.01\n",
        "open_loop!(r,W,W_in,b,data,washout_size+1,washout_size+train_size) ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KEGwbNcqNUm"
      },
      "source": [
        "With the training stage $\\mathbf{r}$ ready, we train the readout by a simple (regularised) regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tH27Utm0qNUn"
      },
      "outputs": [],
      "source": [
        "# Regression: Pseudo-inverse with Tikhonov regularisation (L2 regularisation with γ as the regulariser)\n",
        "R = r[:,train_idx] # Isolate reservoir states at training stage\n",
        "Wout = collect(((R*R'+γ*I)\\ R*data[:,train_idx]')') ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoD8n4xEqNUn"
      },
      "source": [
        "That's it!! You've trained the ESN to predict the original input signal $\\mathbf{x}$. (Isn't it fast?)\n",
        "\n",
        "Of course, if you're interested in other output signals $\\mathbf{y}$ that are functions of $\\mathbf{x}$, you can simply train another `Wout` for the $\\mathbf{y}$ you're interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWhmkGacqNUn"
      },
      "source": [
        "## Testing\n",
        "To evaluate the performance of the training, let's run the ESN in 'close loop' config and compare it with the original signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "lG4r1W_2qNUn",
        "outputId": "f5d59ee1-c838-4fa6-f2ff-b59b6bfd69ef"
      },
      "outputs": [],
      "source": [
        "##  Testing the Echo State Network\n",
        "# Testing stage: Running the ESN in closed-loop from t=washout_size+train_size+1 to t=washout_size+train_size+test_size\n",
        "close_loop!(r,W,W_in,b,data[:,washout_size+train_size],Wout,washout_size+train_size+1,washout_size+train_size+test_size)\n",
        "pred = Wout*r\n",
        "\n",
        "plotting_dim = 2\n",
        "plot_idx = test_idx[1]-100:test_idx[end]\n",
        "plot.scatter(x = tdata[plot_idx],y = data[plotting_dim,plot_idx], name = \"True Data\").\n",
        "  scatter(x = tdata[plot_idx],y =pred[plotting_dim,plot_idx], name = \"ESN\").\n",
        "  scatter(x = tdata[test_idx[1]]*ones(2),y=[-25.0,25.0],mode = \"lines\",\n",
        "          name = \"Training-Testing Split\",line = (color=\"black\",dash=\"dash\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGwaDOogqNUn"
      },
      "source": [
        "You may find that, in close-loop, the prediction will eventially diverge from the signal. That's normal, given the system is chaotic. How to improve the ESN's perforamnce?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuIlboLzqNUn"
      },
      "source": [
        "# More exercises\n",
        "1. What if I want the ESN to map to a function of $\\mathbf{x}$ - $f(\\mathbf{x})$? How can I retrain `Wout` given some data of $f(\\mathbf{x}_t)$?\n",
        "2. What if I only have access to some of the states, say the first two dimension of $\\mathbf{x}$, but not the third? Does the ESN still works? How does that helps you deal with ML problems with partial observation?\n",
        "3. What if I only have access to some measurement of state instead of the state itself (say, $x^2$ instead of $x$)? When will the ESN still work? When it will break?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Julia 1.11.2",
      "language": "julia",
      "name": "julia-1.11"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
